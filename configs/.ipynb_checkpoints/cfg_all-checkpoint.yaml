# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# environment/computational parameters
seed: 32678456782       # random number generator seed (long integer value)
device: cuda            # if you have multiple GPU's, you can use 'cude:4' to specify which GPU to run on, e.g., the 4th
num_workers: 4          # number of CPU cores that load data in parallel. You can set this to the number of logical CPU cores that you have. 

# dataset parameters
data_root: /home/gridsan/nhulkund/beerylab_data_shared/LILA_data # Filepath completed in python script(s)

# training hyperparameters
image_size: [224, 224]
num_epochs: 100           # number of epochs. Each epoch has multiple iterations. In each epoch the model goes over the full dataset once.
batch_size: 64       # number of images that are processed in parallel in every iteration
learning_rate: 0.005    # hyperparameter to adjust the optimizer's learning rate 
weight_decay: 0.01     # hyperparameter for regularization

# dataset
exp_name: snapshot_all
test_dataset: ['Snapshot_Karoo']
train_dataset: ['Snapshot_Kruger', 'Snapshot_Camdeboo', 'Snapshot_Kgalagadi', 'Snapshot_Mountain_Zebra','Snapshot_Serengeti','Snapshot_Enonkishu']
val_dataset: ['Snapshot_Kruger', 'Snapshot_Camdeboo', 'Snapshot_Kgalagadi', 'Snapshot_Mountain_Zebra','Snapshot_Serengeti','Snapshot_Enonkishu']

