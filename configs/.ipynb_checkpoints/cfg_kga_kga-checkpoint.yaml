# Here's where you define experiment-specific hyperparameters.
# You can also create lists and group parameters together into nested sub-parts.
# In Python, this is all read as a dict.

# environment/computational parameters
seed: 32678456782       # random number generator seed (long integer value)
device: cuda            # if you have multiple GPU's, you can use 'cude:4' to specify which GPU to run on, e.g., the 4th
num_workers: 4          # number of CPU cores that load data in parallel. You can set this to the number of logical CPU cores that you have. 

# dataset parameters
data_root: /home/gridsan/nhulkund/beerylab_data_shared/LILA_data # Filepath completed in python script(s)

# training hyperparameters
image_size: [224, 224]
num_epochs: 10          # number of epochs. Each epoch has multiple iterations. In each epoch the model goes over the full dataset once.
batch_size: 16       # number of images that are processed in parallel in every iteration
learning_rate: 0.001  # hyperparameter to adjust the optimizer's learning rate 
weight_decay: 0.01     # hyperparameter for regularization

# dataset
exp_name: snapshot_kga_kga_batchsize=16_learningrate=0.001
test_dataset: /home/gridsan/nhulkund/beerylab_data_shared/LILA_data/metadata_preprocessed/Snapshot_Kgalagadi_test.csv
train_dataset: /home/gridsan/nhulkund/beerylab_data_shared/LILA_data/metadata_preprocessed/Snapshot_Kgalagadi_train.csv
val_dataset: /home/gridsan/nhulkund/beerylab_data_shared/LILA_data/metadata_preprocessed/Snapshot_Kgalagadi_test.csv


